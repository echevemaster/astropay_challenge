# AstroPay Activity Feed API

Unified Activity Feed API to consolidate all financial transactions from multiple microservices into a single source of truth.

## ğŸ—ï¸ Architecture

### Main Components

1. **API Layer (FastAPI)**: RESTful endpoints with automatic validation
2. **Service Layer**: Business logic with separation of concerns
3. **Repository Pattern**: Data access abstraction
4. **Strategy Pattern**: Transaction type-specific processing
5. **Caching Layer (Redis)**: Cache to improve performance
6. **Search Layer (Elasticsearch)**: Advanced full-text search
7. **Event System (Kafka)**: Events for real-time updates
8. **Database (PostgreSQL)**: Primary storage with JSONB for flexible metadata

### Design Patterns Implemented

#### 1. Repository Pattern
- Abstracts data access from business logic
- Facilitates testing and implementation changes
- Location: `app/repositories/transaction_repository.py`

#### 2. Strategy Pattern
- Different strategies to process each transaction type
- Allows extensibility without modifying existing code
- Location: `app/strategies/transaction_strategy.py`

#### 3. Dependency Injection
- Services injected as dependencies
- Facilitates testing and maintenance
- Location: `app/api/dependencies.py`

### Resilience

#### 1. Circuit Breaker Pattern (Implicit)
- Services (Cache, Search, Events) automatically disable if they fail
- Application continues to function in degraded mode
- Health checks monitor the status of each service

#### 2. Graceful Degradation
- If Redis fails, the application works without cache
- If Elasticsearch fails, PostgreSQL search is used
- If Kafka fails, events are omitted but transactions are saved

#### 3. Connection Pooling
- PostgreSQL with configured connection pool
- Prevention of hanging connections with `pool_pre_ping`

#### 4. Retry Logic
- Elasticsearch with automatic retries
- Appropriate timeout configuration

### Performance

#### 1. Caching
- Redis to cache results of frequent queries
- Configurable TTL (default: 5 minutes)
- Automatic invalidation when creating/updating transactions

#### 2. Indexing
- Indexes in PostgreSQL for common queries:
  - `user_id` + `created_at` (composite)
  - `transaction_type` + `status`
  - `product` + `currency`
- Full-text index on `search_content`

#### 3. Hybrid Search
- Elasticsearch for complex and full-text searches
- PostgreSQL for standard filters
- Automatic fallback if Elasticsearch is unavailable

#### 4. Efficient Pagination
- Cursor-based pagination support
- Configurable page limits
- Optimized queries with `OFFSET` and `LIMIT`

#### 5. Async Processing
- Elasticsearch indexing does not block the response
- Events published asynchronously

## ğŸ“Š C4 Flow Diagrams

### Level 1: System Context

```mermaid
graph TB
    subgraph "System Context"
        User[ğŸ‘¤ User<br/>Client consuming the API]
        API[ğŸŒ Activity Feed API<br/>Unified API for financial transactions]
        Microservices[ğŸ”Œ Microservices<br/>Services generating transactions]
    end
    
    User -->|Query and create transactions<br/>HTTPS/REST| API
    Microservices -->|Publish transaction events<br/>Kafka| API
    
    style User fill:#e1f5ff
    style API fill:#fff4e1
    style Microservices fill:#ffe1f5
```

### Level 2: Containers

```mermaid
graph TB
    subgraph "Activity Feed System"
        API[ğŸš€ FastAPI Application<br/>Python/FastAPI<br/>REST API with automatic validation]
        Consumer[ğŸ“¨ Kafka Consumer<br/>Python<br/>Message consumer for indexing]
    end
    
    User[ğŸ‘¤ User]
    Postgres[(ğŸ—„ï¸ PostgreSQL<br/>Primary database with JSONB)]
    Redis[(ğŸ’¾ Redis<br/>In-memory cache)]
    Elasticsearch[(ğŸ” Elasticsearch<br/>Full-text search engine)]
    Kafka[ğŸ“¬ Kafka<br/>Event messaging system]
    
    User -->|HTTPS REST API| API
    API -->|Read and write<br/>SQLAlchemy| Postgres
    API -->|Read and write<br/>Redis Client| Redis
    API -->|Index and search<br/>Elasticsearch Client| Elasticsearch
    API -->|Publish events<br/>Kafka Producer| Kafka
    Consumer -->|Consume messages<br/>Kafka Consumer| Kafka
    Consumer -->|Index transactions<br/>Elasticsearch Client| Elasticsearch
    Consumer -->|Write audit<br/>SQLAlchemy| Postgres
    Consumer -->|Verify idempotency<br/>Redis Client| Redis
    
    style API fill:#fff4e1
    style Consumer fill:#fff4e1
    style Postgres fill:#e1ffe1
    style Redis fill:#ffe1e1
    style Elasticsearch fill:#e1e1ff
    style Kafka fill:#ffe1f5
```

### Flow 1: Create Transaction

```mermaid
sequenceDiagram
    participant U as User
    participant API as FastAPI Router
    participant Auth as Auth Middleware
    participant TS as Transaction Service
    participant Strategy as Transaction Strategy
    participant Repo as Transaction Repository
    participant DB as PostgreSQL
    participant Cache as Cache Service
    participant Search as Search Service
    participant ES as Elasticsearch
    participant Event as Event Service
    participant Kafka as Kafka
    
    U->>API: POST /api/v1/transactions
    API->>Auth: Validate JWT (optional)
    Auth-->>API: user_id
    
    API->>TS: create_transaction(data)
    
    TS->>Strategy: get_strategy(transaction_type)
    Strategy-->>TS: Strategy instance
    
    TS->>Strategy: validate_metadata(metadata)
    Strategy-->>TS: validation result
    
    TS->>Strategy: enrich_metadata(metadata)
    Strategy-->>TS: enriched metadata
    
    TS->>Strategy: build_search_content(data)
    Strategy-->>TS: search_content
    
    TS->>Repo: create(transaction_data)
    Repo->>DB: INSERT transaction
    DB-->>Repo: transaction created
    Repo-->>TS: Transaction object
    
    par Async Indexing
        TS->>Search: index_transaction(transaction)
        Search->>ES: Index document
        ES-->>Search: Success
    and Event Publication
        TS->>Event: publish_transaction_created(transaction)
        Event->>Kafka: Publish event
        Kafka-->>Event: Success
    and Cache Invalidation
        TS->>Cache: delete_pattern(user:*)
        Cache-->>TS: Cache invalidated
    end
    
    TS-->>API: TransactionResponse
    API-->>U: 201 Created
```

### Flow 2: Search/List Transactions

```mermaid
sequenceDiagram
    participant U as User
    participant API as FastAPI Router
    participant Auth as Auth Middleware
    participant TS as Transaction Service
    participant Cache as Cache Service
    participant Redis as Redis
    participant Search as Search Service
    participant ES as Elasticsearch
    participant Repo as Transaction Repository
    participant DB as PostgreSQL
    
    U->>API: GET /api/v1/transactions?filters
    API->>Auth: Validate JWT (optional)
    Auth-->>API: user_id
    
    API->>TS: get_transactions(user_id, filters, pagination)
    
    TS->>Cache: get(cache_key)
    Cache->>Redis: GET key
    Redis-->>Cache: cached result (or null)
    
    alt Cache Hit
        Cache-->>TS: Cached data
        TS-->>API: PaginatedResponse
        API-->>U: 200 OK (from cache)
    else Cache Miss
        alt Has search_query
            TS->>Search: search(user_id, query, filters)
            Search->>ES: Execute search query
            ES-->>Search: transaction_ids, total
            Search-->>TS: IDs and total
            
            loop For each ID
                TS->>Repo: get_by_id(tx_id)
                Repo->>DB: SELECT by id
                DB-->>Repo: Transaction
                Repo-->>TS: Transaction object
            end
        else No search_query
            TS->>Repo: get_by_user_id(user_id, filters, pagination)
            Repo->>DB: SELECT with filters
            DB-->>Repo: transactions, total
            Repo-->>TS: Transactions list
        end
        
        TS->>TS: Build PaginatedResponse
        TS->>Cache: set(cache_key, result)
        Cache->>Redis: SETEX key TTL value
        Redis-->>Cache: Success
        
        TS-->>API: PaginatedResponse
        API-->>U: 200 OK
    end
```

### Flow 3: Get Transaction by ID

```mermaid
sequenceDiagram
    participant U as User
    participant API as FastAPI Router
    participant Auth as Auth Middleware
    participant TS as Transaction Service
    participant Cache as Cache Service
    participant Redis as Redis
    participant Repo as Transaction Repository
    participant DB as PostgreSQL
    
    U->>API: GET /api/v1/transactions/{id}
    API->>Auth: Validate JWT (optional)
    Auth-->>API: user_id
    
    API->>TS: get_transaction(transaction_id)
    
    TS->>Cache: get(cache_key)
    Cache->>Redis: GET transaction:{id}
    Redis-->>Cache: cached result (or null)
    
    alt Cache Hit
        Cache-->>TS: Cached TransactionResponse
        TS->>TS: Validate ownership (if authenticated)
        TS-->>API: TransactionResponse
        API-->>U: 200 OK
    else Cache Miss
        TS->>Repo: get_by_id(transaction_id)
        Repo->>DB: SELECT by id
        DB-->>Repo: Transaction (or null)
        Repo-->>TS: Transaction object
        
        alt Transaction not found
            TS-->>API: None
            API-->>U: 404 Not Found
        else Transaction found
            TS->>TS: Validate ownership (if authenticated)
            TS->>TS: Build TransactionResponse
            TS->>Cache: set(cache_key, response)
            Cache->>Redis: SETEX key TTL value
            Redis-->>Cache: Success
            
            TS-->>API: TransactionResponse
            API-->>U: 200 OK
        end
    end
```

### Flow 4: Health Check

```mermaid
sequenceDiagram
    participant U as User/Monitor
    participant API as FastAPI Router
    participant Health as Health Endpoint
    participant DB as PostgreSQL
    participant Cache as Cache Service
    participant Redis as Redis
    participant Search as Search Service
    participant ES as Elasticsearch
    participant Event as Event Service
    participant Kafka as Kafka
    participant CB as Circuit Breakers
    
    U->>API: GET /api/v1/health
    API->>Health: health_check()
    
    par Check Database
        Health->>DB: SELECT 1
        DB-->>Health: Success/Failure
    and Check Cache
        Health->>Cache: health_check()
        Cache->>Redis: PING
        Redis-->>Cache: PONG
        Cache->>CB: get_redis_breaker().get_state()
        CB-->>Cache: breaker state
        Cache-->>Health: Status + breaker state
    and Check Search
        Health->>Search: health_check()
        Search->>ES: PING
        ES-->>Search: PONG
        Search->>CB: get_elasticsearch_breaker().get_state()
        CB-->>Search: breaker state
        Search-->>Health: Status + breaker state
    and Check Events
        Health->>Event: health_check()
        Event->>Kafka: list_topics()
        Kafka-->>Event: Metadata
        Event->>CB: get_kafka_breaker().get_state()
        CB-->>Event: breaker state
        Event-->>Health: Status + breaker state
    end
    
    Health->>Health: Calculate overall_status
    Note over Health: healthy: all OK<br/>degraded: some degraded<br/>unhealthy: DB failed
    
    Health-->>API: HealthCheckResponse
    API-->>U: 200 OK with detailed status
```

### Flow 5: Kafka Message Consumer

```mermaid
sequenceDiagram
    participant Kafka as Kafka Topic
    participant Consumer as Message Consumer
    participant Cache as Cache Service
    participant Redis as Redis
    participant Search as Search Service
    participant ES as Elasticsearch
    participant Repo as Transaction Repository
    participant DB as PostgreSQL
    participant DLQ as Dead Letter Queue
    
    loop Continuous Consumption
        Kafka->>Consumer: Poll messages (batch)
        
        Consumer->>Consumer: Group messages by batch
        
        loop For each message in batch
            Consumer->>Consumer: Extract transaction_id
            
            Consumer->>Cache: Check idempotency
            Cache->>Redis: GET processed:{transaction_id}
            Redis-->>Cache: processed flag (or null)
            
            alt Already processed (idempotency)
                Cache-->>Consumer: Already processed
                Note over Consumer: Skip message
            else Not processed
                Consumer->>Consumer: Deserialize message
                Consumer->>Consumer: Validate structure
                
                alt Invalid message
                    Consumer->>DLQ: Send to DLQ
                    Note over Consumer: Log error
                else Valid message
                    par Index in Elasticsearch
                        Consumer->>Search: index_transaction(transaction)
                        Search->>ES: Index document (with versioning)
                        ES-->>Search: Success/Failure
                        Search-->>Consumer: Result
                    and Save to DB (audit)
                        Consumer->>Repo: create(transaction)
                        Repo->>DB: INSERT transaction
                        DB-->>Repo: Success
                        Repo-->>Consumer: Transaction
                    end
                    
                    alt Processing successful
                        Consumer->>Cache: Mark as processed
                        Cache->>Redis: SETEX processed:{id} TTL
                        Redis-->>Cache: Success
                        Consumer->>Consumer: Commit offset
                    else Processing error
                        Consumer->>DLQ: Send to DLQ
                        Consumer->>Consumer: Log error
                        Note over Consumer: No commit offset<br/>(automatic retry)
                    end
                end
            end
        end
        
        Consumer->>Kafka: Commit batch offset
    end
```

### Flow 6: Component Diagram (Transaction Service)

```mermaid
graph TB
    subgraph "FastAPI Application"
        Router[ğŸ“¡ Transaction Router<br/>FastAPI Router<br/>Handles REST endpoints]
        Auth[ğŸ” Auth Middleware<br/>JWT Validator<br/>Authentication and authorization]
    end
    
    subgraph "Service Layer"
        TS[âš™ï¸ Transaction Service<br/>Business Logic<br/>Orchestrates operations]
        CacheSvc[ğŸ’¾ Cache Service<br/>Redis Client<br/>Cache management]
        SearchSvc[ğŸ” Search Service<br/>Elasticsearch Client<br/>Full-text search]
        EventSvc[ğŸ“¨ Event Service<br/>Kafka Producer<br/>Event publication]
    end
    
    subgraph "Data Layer"
        Repo[ğŸ—„ï¸ Transaction Repository<br/>Data Access<br/>Data access abstraction]
        Strategy[ğŸ¯ Transaction Strategy<br/>Strategy Pattern<br/>Type-specific processing]
    end
    
    Postgres[(ğŸ—„ï¸ PostgreSQL)]
    Redis[(ğŸ’¾ Redis)]
    Elasticsearch[(ğŸ” Elasticsearch)]
    Kafka[ğŸ“¬ Kafka]
    
    Router -->|Validate JWT| Auth
    Router -->|Call methods| TS
    TS -->|Use cache| CacheSvc
    TS -->|Index and search| SearchSvc
    TS -->|Publish events| EventSvc
    TS -->|Access data| Repo
    TS -->|Process by type| Strategy
    Repo -->|SQL queries| Postgres
    CacheSvc -->|Cache ops| Redis
    SearchSvc -->|Search ops| Elasticsearch
    EventSvc -->|Publish events| Kafka
    
    style Router fill:#fff4e1
    style Auth fill:#fff4e1
    style TS fill:#e1f5ff
    style CacheSvc fill:#e1f5ff
    style SearchSvc fill:#e1f5ff
    style EventSvc fill:#e1f5ff
    style Repo fill:#ffe1f5
    style Strategy fill:#ffe1f5
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker and Docker Compose

### Installation

#### Option A: With Docker (Recommended)

1. **Start infrastructure services**
```bash
docker-compose up -d postgres redis elasticsearch kafka zookeeper
```

2. **Run migrations**
```bash
docker-compose --profile migrate run --rm migrations
```

3. **Start the application**
```bash
docker-compose up api
```

The API will be available at `http://localhost:8000`

#### Option B: Local Development

1. **Create virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Start services with Docker Compose**
```bash
docker-compose up -d postgres redis elasticsearch kafka zookeeper
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env as needed
```

5. **Run migrations**
```bash
alembic upgrade head
```

6. **Start the application**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

> **Note:** For more details on migrations with Docker, see [MIGRATIONS.md](MIGRATIONS.md)

## ğŸ“š API Documentation

Once the application is started, interactive documentation is available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Main Endpoints

#### POST `/api/v1/transactions`
Create a new transaction

**Request Body:**
```json
{
  "user_id": "user123",
  "transaction_type": "card",
  "product": "Card",
  "status": "completed",
  "currency": "USD",
  "amount": 100.50,
  "metadata": {
    "merchant_name": "Amazon",
    "merchant_category": "Retail",
    "card_last_four": "1234",
    "location": "New York, USA"
  }
}
```

#### GET `/api/v1/transactions`
Get transactions with filters and pagination

**Query Parameters:**
- `user_id` (required): User ID
- `transaction_type`: card, p2p, crypto, etc.
- `product`: Card, P2P, Crypto, Earnings
- `status`: completed, pending, failed
- `currency`: USD, EUR, etc.
- `start_date`: Start date (ISO format)
- `end_date`: End date (ISO format)
- `min_amount`: Minimum amount
- `max_amount`: Maximum amount
- `search_query`: Free text search
- `page`: Page number (default: 1)
- `page_size`: Page size (default: 20, max: 100)

**Example:**
```
GET /api/v1/transactions?user_id=user123&product=Card&status=completed&page=1&page_size=20
```

#### GET `/api/v1/transactions/{transaction_id}`
Get a specific transaction by ID

#### GET `/api/v1/health`
Health check of the system and its dependencies

## ğŸ§ª Test Data

### Load Test Data

To generate 1000 synthetic transactions for testing:

#### With Docker (Recommended)

```bash
# Option 1: Use the Docker load script
./load_test_data_docker.sh

# Option 2: Run directly with docker-compose
docker-compose --profile test-data run --rm load_test_data

# Option 3: Run inside the API container
docker-compose exec api python3 generate_test_data.py
```

#### Local Development

```bash
# Option 1: Use the load script
./load_test_data.sh

# Option 2: Run directly
python3 generate_test_data.py
```

This will generate varied transactions with:
- Different types: card, p2p, crypto, top_up, withdrawal, bill_payment, earnings
- Multiple merchants, locations, and peers
- Different statuses, currencies, and amounts
- Complete metadata for each transaction type

Data is generated for `user_id: test_user_123` and you can test all search and filtering functionalities.

### Clean Existing Data

To delete all data from the database and Elasticsearch:

#### With Docker (Recommended)

```bash
# Option 1: Clean all data (PostgreSQL + Elasticsearch + Redis)
./clean_data_docker.sh

# Option 2: Clean and reload in one step
./clean_and_reload_data.sh

# Option 3: Run directly with docker-compose
docker-compose --profile clean run --rm clean_data
```

#### Local Development

```bash
# Run the cleanup script
python3 clean_data.py
```

> **ğŸ“– For more details, see [DOCKER_TEST_DATA.md](DOCKER_TEST_DATA.md)**

## ğŸ§ª Testing

### Run Tests in Docker

```bash
# Run all tests
./run_tests_docker.sh

# With code coverage
./run_tests_docker.sh --cov

# Run a specific file
./run_tests_docker.sh --file tests/test_api_transactions.py

# See all options
./run_tests_docker.sh --help
```

Or directly with docker-compose:

```bash
docker-compose --profile tests run --rm tests
```

> **ğŸ“– For more details, see [TESTING.md](TESTING.md) and [DOCKER_TESTING.md](DOCKER_TESTING.md)**

## ğŸ§ª Usage Examples

### Create Card Payment Transaction
```bash
curl -X POST "http://localhost:8000/api/v1/transactions" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "transaction_type": "card",
    "product": "Card",
    "status": "completed",
    "currency": "USD",
    "amount": 150.00,
    "metadata": {
      "merchant_name": "Starbucks",
      "merchant_category": "Food & Beverage",
      "card_last_four": "5678",
      "location": "San Francisco, CA"
    }
  }'
```

### Create P2P Transaction
```bash
curl -X POST "http://localhost:8000/api/v1/transactions" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "transaction_type": "p2p",
    "product": "P2P",
    "status": "completed",
    "currency": "USD",
    "amount": 50.00,
    "metadata": {
      "peer_name": "John Doe",
      "peer_email": "john@example.com",
      "direction": "sent"
    }
  }'
```

### Search with Filters
```bash
curl "http://localhost:8000/api/v1/transactions?user_id=user123&product=Card&status=completed&search_query=Starbucks&page=1&page_size=20"
```

### Search by Date
```bash
curl "http://localhost:8000/api/v1/transactions?user_id=user123&start_date=2024-01-01T00:00:00Z&end_date=2024-01-31T23:59:59Z"
```

## ğŸ›ï¸ Project Structure

```
astropay_challenge/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Main FastAPI application
â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”œâ”€â”€ database.py              # Database configuration
â”‚   â”œâ”€â”€ models.py                # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py               # Pydantic schemas
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dependencies.py     # Injected dependencies
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ transactions.py  # Transaction routes
â”‚   â”‚       â””â”€â”€ health.py        # Health checks
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ transaction_repository.py  # Repository pattern
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ transaction_service.py    # Business logic
â”‚   â”‚   â”œâ”€â”€ cache_service.py          # Cache service
â”‚   â”‚   â”œâ”€â”€ search_service.py         # Search service
â”‚   â”‚   â””â”€â”€ event_service.py          # Event service
â”‚   â””â”€â”€ strategies/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ transaction_strategy.py   # Strategy pattern
â”œâ”€â”€ alembic/                     # Database migrations
â”œâ”€â”€ docker-compose.yml           # Infrastructure services
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                # Environment variables example
â””â”€â”€ README.md                   # This file
```

## ğŸ” Additional Considerations

### Scalability

1. **Horizontal Scaling**: The application is stateless and can scale horizontally
2. **Database Sharding**: Consider sharding by `user_id` for large volumes
3. **Read Replicas**: Use read replicas to distribute load
4. **CDN**: For static assets if added

### Security

1. **Authentication**: Implement JWT or OAuth2 (currently without auth)
2. **Authorization**: Validate that users only access their transactions
3. **Rate Limiting**: Implement rate limiting (e.g., Redis-based)
4. **Input Validation**: Already implemented with Pydantic
5. **SQL Injection**: Prevented with SQLAlchemy ORM
6. **HTTPS**: Use HTTPS in production

### Monitoring

1. **Logging**: Structured logging with `structlog`
2. **Metrics**: Consider Prometheus + Grafana
3. **Tracing**: Consider OpenTelemetry for distributed tracing
4. **Alerting**: Configure alerts for critical services

### Testing

1. **Unit Tests**: For services and repositories
2. **Integration Tests**: For API endpoints
3. **Load Tests**: To validate performance under load
4. **Contract Tests**: To validate API contracts

**Run Tests:**

```bash
# In Docker (recommended)
./run_tests_docker.sh

# With coverage
./run_tests_docker.sh --cov

# See complete documentation
cat TESTING.md
cat DOCKER_TESTING.md
```

### Future Optimizations

1. **Materialized Views**: For complex aggregations
2. **Event Sourcing**: For complete audit trail
3. **CQRS**: Separate commands and queries
4. **GraphQL**: For more flexible queries
5. **WebSockets**: For real-time updates to frontend

## ğŸ“ Implementation Notes

- External services (Redis, Elasticsearch, Kafka) are optional and the application works without them
- Metadata is flexible using JSONB, allowing different fields per transaction type
- Full-text search works in both PostgreSQL and Elasticsearch
- Events are published asynchronously and do not block transaction creation

## ğŸ“š Complete Documentation

This section contains links to all available project documentation, organized by category.

### ğŸ”Œ API and Usage Guides

- **[API_USAGE.md](API_USAGE.md)** - Complete API usage guide with examples of all endpoints
- **[AUTHENTICATION.md](AUTHENTICATION.md)** - Documentation on JWT authentication and how to obtain tokens
- **[CARD_PAYMENT_GUIDE.md](CARD_PAYMENT_GUIDE.md)** - Specific guide for working with card transactions
- **[METADATA_FILTERS_GUIDE.md](METADATA_FILTERS_GUIDE.md)** - How to filter transactions using metadata fields
- **[DATE_FILTER_GUIDE.md](DATE_FILTER_GUIDE.md)** - Guide for filtering transactions by date range

### ğŸ” Search and Elasticsearch

- **[SEARCH_GUIDE.md](SEARCH_GUIDE.md)** - Complete search guide: free text, advanced filters, and Elasticsearch usage
- **[ELASTICSEARCH_SETUP.md](ELASTICSEARCH_SETUP.md)** - Elasticsearch configuration and installation
- **[ELASTICSEARCH_PRIMARY.md](ELASTICSEARCH_PRIMARY.md)** - How to use Elasticsearch as primary data source

### ğŸ§ª Testing

- **[TESTING.md](TESTING.md)** - Complete testing guide: how to run tests, write new tests, and structure
- **[DOCKER_TESTING.md](DOCKER_TESTING.md)** - Run tests in Docker with all available options
- **[DOCKER_TEST_DATA.md](DOCKER_TEST_DATA.md)** - How to generate and load test data using Docker

### ğŸ—„ï¸ Database and Migrations

- **[MIGRATIONS.md](MIGRATIONS.md)** - Complete database migrations guide with Alembic

### ğŸ“¨ Messaging and Events

- **[MESSAGE_CONSUMER.md](MESSAGE_CONSUMER.md)** - Kafka consumer documentation: idempotency, batch processing, DLQ, data enrichment

### ğŸ›¡ï¸ Resilience and Performance

- **[RESILIENCE.md](RESILIENCE.md)** - Implemented resilience patterns: circuit breakers, rate limiting, retries, timeouts, health checks

## ğŸ¤ Contribution

This is a technical assessment project. For production, consider:
- Complete tests
- CI/CD pipeline
- More detailed documentation
- Robust security configuration
- Monitoring and alerts

## ğŸ“„ License

This project is part of a technical assessment for AstroPay.
